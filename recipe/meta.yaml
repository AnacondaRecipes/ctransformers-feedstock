{% set name = "ctransformers" %}
{% set version = "0.2.27" %}
{% set build_number = 2 %}

package:
  name: {{ name }}
  version: {{ version }}

source:
  url: https://github.com/marella/{{ name }}/archive/refs/tags/v{{ version }}.tar.gz
  sha256: 62b556c0b1d355cdb681108268ee626e6ee6f39f4abc1429606583f348c55ee8
  patches:
    - patches/0001-Load-library-from-conda-prefix.patch
    - patches/0002-Fix-install-path.patch
    - patches/0003-Make-FindCUDAToolkit-required-when-building-for-GPU.patch

build:
  # Use a build number difference to ensure that the GPU
  # variant is slightly preferred by conda's solver, so that it's preferentially
  # installed where the platform supports it.
  number: {{ build_number + 100 }}  # [ctransformers_variant == "gpu"]
  number: {{ build_number }}        # [ctransformers_variant == "cpu"]
  skip: true # [skip_cuda_prefect and (gpu_variant or "").startswith('cuda')]
  skip: true  # [win]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}  # [ctransformers_variant == 'gpu' and not osx]
    - cmake
    - ninja
    - patch     # [not win]
    - m2-patch  # [win]

outputs:
  - name: ctransformers
    script: build_base.sh
    build:
      string: gpu_cuda{{ cudatoolkit | replace('.', '') }}py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}  # [ctransformers_variant == "gpu" and linux]
      string: gpu_mps_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                      # [ctransformers_variant == "gpu" and osx]
      string: cpu_py{{ CONDA_PY }}h{{PKG_HASH}}_{{ PKG_BUILDNUM }}                                          # [ctransformers_variant == "cpu"]
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}  # [ctransformers_variant == 'gpu' and not osx]
        - cmake
        - ninja
        - patch     # [not win]
        - m2-patch  # [win]
      host:
        - python
        # setuptools build backend is used if CT_WHEEL=1
        - setuptools
        - wheel
        - pip
        - cuda-toolkit {{ cudatoolkit }}*  # [ctransformers_variant == 'gpu' and not osx]
        - cuda-version                     # [ctransformers_variant == 'gpu' and not osx]
      run:
        - python
        - huggingface_hub
        - py-cpuinfo >=9.0.0,<10.0.0
        - {{ pin_compatible('cuda-toolkit', max_pin='x.x') }}  # [ctransformers_variant == 'gpu' and not osx]
        - cuda-version                                         # [ctransformers_variant == 'gpu' and not osx]

    test:
      imports:
        - ctransformers
      source_files:
        - tests
      requires:
        - pip
        - pytest
      commands:
        - pip check
        # The default cache is ~/.cache/hub and ~/.cache/assets
        # We don't want to fill the host's disk with test data.
        - export HUGGINGFACE_HUB_CACHE=huggingface_cache            # [not win]
        - export HUGGINGFACE_ASSETS_CACHE=huggingface_assets_cache  # [not win]
        - set HUGGINGFACE_HUB_CACHE="huggingface_cache"             # [win]
        - set HUGGINGFACE_ASSETS_CACHE="huggingface_assets_cache"   # [win]
        - nvcc --version                                            # [ctransformers_variant == "gpu" and (linux and x86_64)]
        - python -c 'import ctransformers.llm; ctransformers.llm.load_library()'
        - pytest -v tests
        # Check also pytest tests --lib {avx,avx2,basic,cuda,none}
        # Verify supported instruction sets on your machine
        - cat /proc/cpuinfo | grep flags | uniq                     # [ctransformers_variant == "gpu" and (linux and x86_64)]
        - pytest tests --lib avx2                                    # [ctransformers_variant == "gpu" and (linux and x86_64)]
        # This will actually download model and will force GPU.
        # The models size is 2.87G+.
        #- python -c 'from ctransformers import AutoModelForCausalLM; llm = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7B-GGML", gpu_layers=50); print(llm("AI is going to"))'

  - name: ctransformers-{{ ctransformers_variant }}
    build:
      noarch: generic
    requirements:
      run:
        - ctransformers ={{ version }}={{ ctransformers_variant }}*

    test:
      imports:
        - ctransformers
      source_files:
        - tests
      requires:
        - pip
        - pytest
      commands:
        - pip check
        # The default cache is ~/.cache/hub and ~/.cache/assets
        # We don't want to fill the host's disk with test data.
        - export HUGGINGFACE_HUB_CACHE=huggingface_cache            # [not win]
        - export HUGGINGFACE_ASSETS_CACHE=huggingface_assets_cache  # [not win]
        - set HUGGINGFACE_HUB_CACHE="huggingface_cache"             # [win]
        - set HUGGINGFACE_ASSETS_CACHE="huggingface_assets_cache"   # [win]
        - nvcc --version                                            # [ctransformers_variant == "gpu" and (linux and x86_64)]
        - python -c 'import ctransformers.llm; ctransformers.llm.load_library()'
        - pytest -v tests
        # Check also pytest tests --lib {avx,avx2,basic,cuda,none}
        # Verify supported instruction sets on your machine
        - cat /proc/cpuinfo | grep flags | uniq                     # [ctransformers_variant == "gpu" and (linux and x86_64)]
        - pytest tests --lib avx2                                    # [ctransformers_variant == "gpu" and (linux and x86_64)]
        # This will actually download model and will force GPU.
        # The models size is 2.87G+.
        #- python -c 'from ctransformers import AutoModelForCausalLM; llm = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7B-GGML", gpu_layers=50); print(llm("AI is going to"))'


about:
  home: https://github.com/marella/ctransformers
  license: MIT
  license_file: LICENSE
  license_family: MIT
  description: Python bindings for the Transformer models implemented in C/C++ using GGML library.
  summary: Python bindings for the Transformer models implemented in C/C++ using GGML library.
  doc_url: https://github.com/marella/ctransformers#documentation
  dev_url: https://github.com/marella/ctransformers

extra:
  recipe-maintainers:
    - JeanChristopheMorinPerso
